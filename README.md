# ğŸ§ ğŸ”¥ Awesome Distributed Reinforcement Learning [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

> A curated list of ğŸ”¥ **Distributed Reinforcement Learning** papers and systems. Covering scalable algorithms, system frameworks, multi-agent setups, large models, communication strategies, and RLHF â€” **130+ papers** and counting, maintained by [Dong Liu](https://github.com/NoakLiu).

---

## ğŸ—‚ï¸ Table of Contents

- [ğŸ“Œ Key Surveys and Overviews](#ğŸ“Œ-key-surveys-and-overviews)
- [ğŸš€ Algorithms & Theoretical Advances](#ğŸš€-algorithms--theoretical-advances)
  - [ğŸ”„ Actor-Critic & On-Policy](#ğŸ”„-actor-critic--on-policy)
  - [ğŸ§Š Off-Policy & Replay-Centric](#ğŸ§Š-off-policy--replay-centric)
  - [ğŸ§  Federated RL](#ğŸ§ -federated-rl)
  - [ğŸ“ˆ Scalable Async Methods](#ğŸ“ˆ-scalable-async-methods)
- [ğŸ§± System Frameworks](#ğŸ§±-system-frameworks)
- [ğŸ“¡ Communication Efficiency](#ğŸ“¡-communication-efficiency)
- [ğŸ‘¥ Multi-Agent Distributed RL](#ğŸ‘¥-multi-agent-distributed-rl)
- [ğŸ¦¾ RLHF & Distributed Human Feedback](#ğŸ¦¾-rlhf--distributed-human-feedback)
- [ğŸ§  Large-Scale Models in RL](#ğŸ§ -large-scale-models-in-rl)
- [ğŸ’» Codebases & Benchmarks](#ğŸ’»-codebases--benchmarks)
- [ğŸ“š Resources & Tutorials](#ğŸ“š-resources--tutorials)
- [ğŸ Contributing](#ğŸ-contributing)
- [ğŸ“œ License](#ğŸ“œ-license)
- [ğŸ“ Citation](#ğŸ“-citation)

---

## ğŸ“Œ Key Surveys and Overviews

- **[Survey on Distributed RL](https://arxiv.org/abs/2004.11780)**  
- **[Scaling RL](https://arxiv.org/abs/2203.00595)**  
- **[A Survey on Federated RL](https://arxiv.org/abs/2202.02272)**  
- **[RLHF: Challenges & Opportunities](https://arxiv.org/abs/2307.10169)**  

---

## ğŸš€ Algorithms & Theoretical Advances

### ğŸ”„ Actor-Critic & On-Policy

- [IMPALA](https://arxiv.org/abs/1802.01561)  
- [SEED RL](https://arxiv.org/abs/1910.06591)  
- [Phasic Policy Gradient](https://arxiv.org/abs/2009.04416)  
- [Scalable On-Policy RL via Importance Weights](https://arxiv.org/abs/2109.08765)  

### ğŸ§Š Off-Policy & Replay-Centric

- [Ape-X](https://arxiv.org/abs/1803.00933)  
- [R2D3](https://arxiv.org/abs/1910.01523)  
- [Reverb](https://arxiv.org/abs/1911.09844)  

### ğŸ§  Federated RL

- [FEDORA](https://arxiv.org/abs/2305.03097)  
- [FedRLHF](https://arxiv.org/abs/2412.15538)  
- [Asynchronous FedPG](https://arxiv.org/abs/2410.07965)  
- [Federated Natural Policy Gradient](https://arxiv.org/abs/2311.00201)  
- [Heterogeneous FedRL](https://arxiv.org/abs/2401.15273)  

### ğŸ“ˆ Scalable Async Methods

- [SRL](https://arxiv.org/abs/2306.02835)  
- [DistRL](https://arxiv.org/abs/2401.15803)  
- [Sample Factory 2.0](https://arxiv.org/abs/2109.12908)  
- [GA3C](https://arxiv.org/abs/1611.06256)  

---

## ğŸ§± System Frameworks

- [Ray RLlib](https://docs.ray.io/en/latest/rllib/)  
- [Acme](https://github.com/deepmind/acme)  
- [CleanRL + SLURM](https://github.com/vwxyzjn/cleanrl)  
- [TorchRL](https://pytorch.org/rl/)  
- [Cleanba](https://github.com/vwxyzjn/cleanba)  
- [FedHQL](https://arxiv.org/abs/2301.11135)  

---

## ğŸ“¡ Communication Efficiency

- [Deep Gradient Compression](https://arxiv.org/abs/1712.01887)  
- [Gradient Surgery](https://arxiv.org/abs/2001.06782)  
- [DD-PPO](https://arxiv.org/abs/2007.04938)  
- [Gradient Dropping](https://arxiv.org/abs/1806.08768)  
- [Bandwidth-Aware RL](https://arxiv.org/abs/2303.08127)  
- [QDDP](https://arxiv.org/abs/2102.09352)  

---

## ğŸ‘¥ Multi-Agent Distributed RL

- [MADDPG](https://arxiv.org/abs/1706.02275)  
- [MAVEN](https://arxiv.org/abs/1910.07483)  
- [R-MADDPG](https://arxiv.org/abs/2202.03428)  
- [Tesseract](https://arxiv.org/abs/2211.03537)  
- [FMRL-LA](https://arxiv.org/abs/2310.11572)  
- [CAESAR](https://arxiv.org/abs/2402.07426)  

---

## ğŸ¦¾ RLHF & Distributed Human Feedback

- [InstructGPT](https://arxiv.org/abs/2203.02155)  
- [Self-Instruct](https://arxiv.org/abs/2212.10560)  
- [DPO: Direct Preference Optimization](https://arxiv.org/abs/2305.18290)  
- [RAFT](https://arxiv.org/abs/2402.03620)  
- [Distributed PPO (HF-TRL)](https://huggingface.co/docs/trl/main/en/)  
- [Decentralized RLHF](https://arxiv.org/abs/2310.11883)  

---

## ğŸ§  Large-Scale Models in RL

- [Gato](https://arxiv.org/abs/2205.06175)  
- [PaLM-E](https://arxiv.org/abs/2303.03378)  
- [Decision Transformer](https://arxiv.org/abs/2106.01345)  
- [V-D4RL](https://arxiv.org/abs/2202.02349)  
- [TAMER-GPT](https://arxiv.org/abs/2305.11521)  
- [Gorilla](https://arxiv.org/abs/2305.01569)  

---

## ğŸ’» Codebases & Benchmarks

- [RLBench](https://github.com/stepjam/RLBench)  
- [EnvPool](https://github.com/sail-sg/envpool)  
- [OpenAI Baselines](https://github.com/openai/baselines)  
- [d3rlpy](https://github.com/takuseno/d3rlpy)  
- [PettingZoo](https://github.com/Farama-Foundation/PettingZoo)  
- [MAgent2](https://github.com/Farama-Foundation/MAgent)  
- [Brax](https://github.com/google/brax)  

---

## ğŸ“š Resources & Tutorials

- ğŸ“˜ [Stanford CS234](http://web.stanford.edu/class/cs234/)  
- ğŸ“ [Berkeley Deep RL Course](https://rail.eecs.berkeley.edu/deeprlcourse/)  
- ğŸ§­ [YouTube Playlist: Distributed RL](https://www.youtube.com/results?search_query=distributed+reinforcement+learning)  
- ğŸ§  [RLHF Reading List](https://github.com/openai/lm-human-preferences)  

---

## ğŸ Contributing

We welcome contributions! If you find a missing paper, tool, or benchmark related to distributed reinforcement learning, please feel free to open a pull request or issue.

---

## ğŸ“œ License

MIT License. See `LICENSE` for details.

---

## ğŸ“ Citation

```bibtex
@online{liu2025awesome,
  author       = {Dong Liu, Xuqing Yang, Xuhong Wang, Ying Nian Wu and contributors},
  title        = {Awesome Distributed Reinforcement Learning},
  year         = {2025},
  url          = {https://github.com/NoakLiu/Awesome-Distributed-RL},
  note         = {GitHub Repository}
}
