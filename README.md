# ğŸ§ ğŸ”¥ Awesome Distributed Reinforcement Learning [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

> A curated list of ğŸ”¥ **Distributed Reinforcement Learning** papers and systems. Covering scalable algorithms, system frameworks, multi-agent setups, large models, communication strategies, and RLHF â€” **120+ papers** and counting.

---

## ğŸ—‚ï¸ Table of Contents

- [ğŸ“Œ Key Surveys and Overviews](#ğŸ“Œ-key-surveys-and-overviews)
- [ğŸš€ Algorithms & Theoretical Advances](#ğŸš€-algorithms--theoretical-advances)
- [ğŸ§± System Frameworks](#ğŸ§±-system-frameworks)
- [ğŸ“¡ Communication Efficiency](#ğŸ“¡-communication-efficiency)
- [ğŸ‘¥ Multi-Agent Distributed RL](#ğŸ‘¥-multi-agent-distributed-rl)
- [ğŸ¦¾ RLHF & Distributed Human Feedback](#ğŸ¦¾-rlhf--distributed-human-feedback)
- [ğŸ§  Large-Scale Models in RL](#ğŸ§ -large-scale-models-in-rl)
- [ğŸ’» Codebases & Benchmarks](#ğŸ’»-codebases--benchmarks)
- [ğŸ“š Resources & Tutorials](#ğŸ“š-resources--tutorials)

---

## ğŸ“Œ Key Surveys and Overviews

- **[Survey on Distributed RL](https://arxiv.org/abs/2004.11780)** â€” Overview of algorithms, architectures, and challenges in Distributed Reinforcement Learning.
- **[Scaling RL](https://arxiv.org/abs/2203.00595)** â€” Challenges and insights from industrial-scale applications.

---

## ğŸš€ Algorithms & Theoretical Advances

| Title | Year | Link |
|-------|------|------|
| IMPALA: Scalable Distributed Deep-RL | 2018 | [arXiv](https://arxiv.org/abs/1802.01561) |
| Ape-X: Distributed Prioritized Replay | 2018 | [arXiv](https://arxiv.org/abs/1803.00933) |
| SEED RL | 2020 | [arXiv](https://arxiv.org/abs/1910.06591) |
| Muesli | 2021 | [arXiv](https://arxiv.org/abs/2104.06159) |
| Accelerated Methods for Distributed RL | 2022 | [arXiv](https://arxiv.org/abs/2203.09511) |
| **SRL: Scaling Distributed Reinforcement Learning to Over Ten Thousand Cores** | 2023 | [arXiv](https://arxiv.org/abs/2306.02835) |
| **Federated Ensemble-Directed Offline Reinforcement Learning (FEDORA)** | 2023 | [arXiv](https://arxiv.org/abs/2305.03097) |
| **Federated Natural Policy Gradient and Actor Critic Methods** | 2023 | [arXiv](https://arxiv.org/abs/2311.00201) |
| **Loss- and Reward-Weighting for Efficient Distributed Reinforcement Learning** | 2024 | [arXiv](https://arxiv.org/abs/2311.01354) |
| **Asynchronous Federated Reinforcement Learning with Policy Gradient Updates** | 2024 | [arXiv](https://arxiv.org/abs/2410.07965) |
| **Finite-Time Analysis of On-Policy Heterogeneous Federated RL** | 2024 | [arXiv](https://arxiv.org/abs/2401.15273) |

---

## ğŸ§± System Frameworks

- **[Ray RLlib](https://docs.ray.io/en/latest/rllib/index.html)** â€” Scalable and general-purpose distributed RL framework.
- **[Acme](https://github.com/deepmind/acme)** â€” Modular RL framework by DeepMind.
- **[CleanRL + SLURM](https://github.com/vwxyzjn/cleanrl)** â€” Simple baseline with distributed support.
- **[TorchRL](https://pytorch.org/rl/)** â€” Native PyTorch support for distributed rollouts.
- **[Cleanba](https://github.com/vwxyzjn/cleanba)** â€” A reproducible and efficient distributed RL platform (2023).
- **[DistRL](https://arxiv.org/abs/2401.15803)** â€” Asynchronous distributed RL framework for on-device control agents (2024).
- **[FedHQL](https://arxiv.org/abs/2301.11135)** â€” Federated Heterogeneous Q-Learning for black-box agents (2023).

---

## ğŸ“¡ Communication Efficiency

- **[Deep Gradient Compression](https://arxiv.org/abs/1712.01887)** â€” Key technique for bandwidth-efficient distributed optimization.
- **[Gradient Surgery](https://arxiv.org/abs/2001.06782)** â€” Improves multi-task and multi-agent communication efficiency.
- **[Bandwidth-Aware RL](https://arxiv.org/abs/2303.08127)** â€” Explicit tradeoffs between learning speed and communication cost.

---

## ğŸ‘¥ Multi-Agent Distributed RL

- **[MADDPG](https://arxiv.org/abs/1706.02275)** â€” Centralized training with decentralized execution.
- **[MAVEN](https://arxiv.org/abs/1910.07483)** â€” Macro-action exploration.
- **[R-MADDPG](https://arxiv.org/abs/2202.03428)** â€” Resource-aware extensions for MARL.
- **[Tesseract](https://arxiv.org/abs/2211.03537)** â€” Decentralized, scalable MARL with communication learning.
- **[FMRL-LA: Cost-Efficient Federated Multi-Agent RL](https://arxiv.org/abs/2310.11572)** â€” Federated MARL with learnable aggregation (2023).
- **[CAESAR: Federated RL in Heterogeneous MDPs](https://arxiv.org/abs/2402.07426)** â€” Convergence-aware sampling with screening for diverse environments (2024).

---

## ğŸ¦¾ RLHF & Distributed Human Feedback

- **[InstructGPT](https://arxiv.org/abs/2203.02155)** â€” Scaling human feedback training.
- **[RLHF System Design](https://arxiv.org/abs/2307.10169)** â€” Open-source scalable RLHF pipelines.
- **[Distributed PPO for RLHF](https://huggingface.co/docs/trl/main/en/)** â€” HuggingFace TRL with DDP.
- **[FedRLHF: Privacy-Preserving Federated RLHF](https://arxiv.org/abs/2412.15538)** â€” Convergence-guaranteed federated framework for personalized RLHF (2024).

---

## ğŸ§  Large-Scale Models in RL

- **[Gato](https://arxiv.org/abs/2205.06175)** â€” Generalist agent from DeepMind.
- **[V-D4RL](https://arxiv.org/abs/2202.02349)** â€” Large vision-language models for offline RL.
- **[Decision Transformer](https://arxiv.org/abs/2106.01345)** â€” Sequence modeling for reward-conditioned behavior.
- **[XLand](https://deepmind.com/research/publications/xland)** â€” Massive parallel environments for training open-ended agents.

---

## ğŸ’» Codebases & Benchmarks

- ğŸ§ª [RL Bench](https://github.com/stepjam/RLBench)
- ğŸ› ï¸ [EnvPool (Ultra-Fast Vectorized Env)](https://github.com/sail-sg/envpool)
- ğŸ§° [OpenAI Baselines (DDPG/A3C/TRPO)](https://github.com/openai/baselines)
- ğŸ” [d3rlpy (Offline RL)](https://github.com/takuseno/d3rlpy)
- ğŸŒ [PettingZoo (MARL Env)](https://github.com/Farama-Foundation/PettingZoo)

---

## ğŸ“š Resources & Tutorials

- ğŸ“˜ [Stanford CS234](http://web.stanford.edu/class/cs234/index.html) â€” Reinforcement Learning
- ğŸ“ [Berkeley Deep RL Course](https://rail.eecs.berkeley.edu/deeprlcourse/) â€” Lectures and assignments
- ğŸ§­ [DistRL YouTube Playlist](https://www.youtube.com/results?search_query=distributed+reinforcement+learning)

---

## ğŸ Contributing

Got a new awesome paper or codebase? Open a pull request! We welcome contributions.

---

## ğŸ“œ License

Distributed under the MIT License. See `LICENSE` for more information.

---
## Citation
```bash
@online{liu2025awesome,
  author       = {Dong Liu, Xuqing Yang, Xuhong Wang, Ying Nian Wu},
  title        = {Awesome Distributed Reinforcement Learning},
  year         = {2025},
  url          = {https://github.com/NoakLiu/Awesome-Distributed-RL},
  note         = {GitHub Repository}
}
```
